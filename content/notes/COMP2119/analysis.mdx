---
title: Algorithm Analysis
---

We usually determine the efficiency of an algorithm by analyzing its time and space complexity.


<Block variant="primary" title="Size of input">
An **input** is a data structure that is given to an algorithm to solve a problem.
The **size** of an input is the number $\mathbf{n}$ of elements in the input.
</Block>



## How do we assess time complexity?


When we analyize time complexity, we are interested in the efficiency of an algorithm as a function of the size of the input.

As different machines have varying speeds of processors, time is not a good fit for measuring algorithm efficiency.

We can consider the number of operations an algorithm performs relative to the size of the input, which will isolate the algorithm from the performing machines.

However, different operations have varying efficiencies, so instead we consider the **growth rate** of the **total number of operations** as a function of the input size. The analysis of such is called **Asymptotic analysis**.



<Block variant="primary" title="Time complexity">
The **rate of growth** of the **total number of operations** an algorithm performs relative to **size of input**.
</Block>



<Block variant="primary" title="Space complexity">
The **rate of growth** of the **total amount of memory** an algorithm uses relative to **size of input**.
</Block>



## Asymptotic notation




<Block variant="secondary" title="Growth rate / Complexity: The basis of asymptotic notations">
We are defining a certain function $\mathbf{T(n)}$ to represent the **total number of operations** with respect to $\mathbf{n}$ (the size of the input).
When we say $T(n)$ is $A$ of $(g(n))$ (or $T(n)\in A(g(n))$), we are saying that the **growth rate** of $T(n)$ is bounded by $A(g(n))$ under a specific mathematical inequality defined by $A$.
</Block>






<Block variant="primary" title="General definition of asymptotic notations">
```math
T(n) \in A(g(n))\ \text{iff}\ \exists\ c > 0:
```
```math
T(n) \simeq c\cdot g(n)\ \forall\ n \geq n_0 > 0
```
```math
T(n) \in a(g(n))\ \text{if}\ \forall\ c > 0\ \exists\ n_0 \geq 0:
```
```math
T(n) \sim c\cdot g(n)\ \forall\ n \geq n_0
```
Note that the $\in$ is interchangable with $=$. The meaning does not really matter.
<BlockSep />
To prove non-strict boundaries, we need to show the existence of constants $c, n_0$ that satisfies the inequality.
[Asym 1](FIXME-eg:asym_1) [Asym 2](FIXME-eg:asym_2) [Asym 3](FIXME-eg:asym_3)
</Block>














<Block variant="primary" title="Definition of all asymptotic notations">
</Block>








<Block variant="knowledge" title="Growth rate functions">
We use the likeness of $g(n)$ to describe the complexity of $T(n)$. The following gives the common growth rate functions (increasing order of growth):
```math
1 < \log n < \sqrt{n} < n < n\log n < n^2 < n^3 < 2^n < n! < n^n
```
Note that the higher the growth rate, the more complex the algorithm.
Note the following growth rate equivilencies:

    - $\log(n!)=\Theta(n\log(n))$ (prove not needed, related to [lower bound of sorting algorithms](FIXME-thm:lower_bound_sort))
    - $\Theta(\log_2n)=\Theta(\log_{10}n)$ as $\log_2n = \frac{\log n}{\log 2} = c\times\log n$
</Block>








<Block variant="secondary" title="Identifying asymptotic growths">
*Non-strict*: Identify **the term with the highest growth rate** in $T(n)$ would be $g(n)$.
*Strict*: Choose the **next / previous** growth rate function with reference to the list depending on if it's upper / lower.
You can use the same logic to determine if some $f(n)$ is $A/a$ of $g(n)$.
Note that as the defintions are specified by an **inequality**, there could be **multiple satisfying** $g(n)$ for the same $T(n)$ and all of them are valid. However, keep in mind the only useful $g(n)$ for analysis would be closest to the condition boundary.
If the function functuates, there is not a consistent growth rate unless a boundary is specified.
<BlockSep />
[Iasym 1](FIXME-eg:iasym_1)
</Block>



### Big Theta of resursive relations


When tasked to find find the asymptotic notation of a recursive relation, we are required to give the Big Theta notation of the relation.

The straight forward way is to find the closed-form expression and identify the growth rate. However, this is not always possible.


<Block variant="knowledge" title="Useful growth rates">

    - **Sum of square series**: $\sum_{i=1}^{k}f(n, i)^2\in \Theta(n^3)$
    - **Sum of series**: $\sum_{i=1}^{k}f(n, i)\in \Theta(n^2)$
</Block>



## Time complexity of code



<Block variant="knowledge" title="Operation of time complexities">
By definition:
```math
O(g)+O(g)=k\times O(g)=O(g)
```
```math
O(g)\times O(g)=O(g\times g)
```
```math
O(g_1)+O(g_2)=O(g_2),     g_2 > g_1\text{ in terms of order of growth}
```
</Block>


Our goal is to examine how the runtime grows with respect to the input size. Consider the following example:

```python
# 1
for num in arr:                     # n
if num > max_val:               # n
max_val = num               # n
return max_val                      # 1
```

It should be acknowledged that each operation / line would take different amounts of time to compute. However, as we are interested in the runtime $T(n)$'s growth rate, we can treat all operations to take $1$ unit of time.

Hence, $T(n)=O(1)+O(1)+O(1)+O(1)+O(n)+O(n)+O(n)+O(1)=O(n)$
















<Block variant="secondary" title="Typical growth rates and common occurance">
</Block>